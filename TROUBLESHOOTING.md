# üîß Troubleshooting - Ollama Cloud Run

## ‚ùå Problema Identificado: N√£o consegue executar chamadas ao Ollama

### üîç Diagn√≥stico Completo

#### **Problema 1: Conflito de Vari√°vel OLLAMA_HOST**
```yaml
# ‚ùå ERRADO - For√ßando valor no YAML
env:
- name: OLLAMA_HOST
  value: 0.0.0.0:8080
```

**Causa:** O Cloud Run injeta automaticamente a vari√°vel `PORT` (geralmente 8080), e o Dockerfile j√° est√° configurado para usar essa vari√°vel. Quando voc√™ for√ßa `OLLAMA_HOST` no YAML, est√° sobrescrevendo essa configura√ß√£o inteligente.

**Solu√ß√£o:** **REMOVA** a vari√°vel `OLLAMA_HOST` do YAML do Cloud Run. Deixe o Dockerfile gerenciar isso.

---

#### **Problema 2: Startup Probe Agressivo**
```yaml
# ‚ùå ERRADO
startupProbe:
  timeoutSeconds: 240
  periodSeconds: 240
  failureThreshold: 1  # Falha ap√≥s 1 tentativa!
```

**Causa:** O Ollama demora para carregar modelos grandes. Com `failureThreshold: 1`, se o servi√ßo n√£o responder em 4 minutos, ele j√° √© considerado falho e reiniciado.

**Solu√ß√£o:** Aumentar o `failureThreshold` e reduzir o `periodSeconds`:

```yaml
# ‚úÖ CORRETO
startupProbe:
  timeoutSeconds: 10
  periodSeconds: 10
  failureThreshold: 30  # 30 tentativas = 5 minutos total
  httpGet:
    path: /api/tags
    port: 8080
```

---

#### **Problema 3: Health Check com Porta Fixa**
No Dockerfile original:
```dockerfile
# ‚ùå ERRADO - Porta hardcoded
HEALTHCHECK CMD curl -f http://localhost:11434/api/tags
```

**Solu√ß√£o:** Usar a vari√°vel `PORT`:
```dockerfile
# ‚úÖ CORRETO - Usa a PORT do Cloud Run
HEALTHCHECK CMD curl -f http://localhost:${PORT:-11434}/api/tags
```

---

## üöÄ Como Resolver

### **Op√ß√£o 1: Redeploy com Script Automatizado (RECOMENDADO)**

```bash
# Tornar o script execut√°vel (se ainda n√£o fez)
chmod +x redeploy-fix.sh

# Fazer redeploy no ambiente de desenvolvimento
./redeploy-fix.sh sandbox
```

O script ir√°:
1. ‚úÖ Remover a vari√°vel `OLLAMA_HOST` conflitante
2. ‚úÖ Configurar a porta correta (8080)
3. ‚úÖ Ajustar o timeout e concurrency
4. ‚úÖ Testar a conectividade automaticamente

---

### **Op√ß√£o 2: Atualizar Manualmente via gcloud**

```bash
gcloud run deploy brh-ollama-dev \
  --project=brh-dev-469211 \
  --region=us-east4 \
  --image=us-east4-docker.pkg.dev/brh-dev-469211/cloud-run-source-deploy/brh-ollama-dev/brh-ollama-dev:latest \
  --platform=managed \
  --cpu=6 \
  --memory=16Gi \
  --no-cpu-throttling \
  --min-instances=1 \
  --max-instances=3 \
  --timeout=600s \
  --concurrency=80 \
  --port=8080 \
  --set-env-vars="ENVIRONMENT=sandbox,OLLAMA_KEEP_ALIVE=12h" \
  --allow-unauthenticated
```

**IMPORTANTE:** Note que **N√ÉO** estamos definindo `OLLAMA_HOST` aqui!

---

### **Op√ß√£o 3: Editar YAML Manualmente**

Se voc√™ precisa editar o YAML diretamente no Console do GCP:

1. V√° para Cloud Run > brh-ollama-dev > EDITAR E IMPLANTAR NOVA REVIS√ÉO

2. **REMOVA** estas linhas da se√ß√£o `env`:
```yaml
# ‚ùå REMOVER ISTO
- name: OLLAMA_HOST
  value: 0.0.0.0:8080
```

3. Na se√ß√£o de **Startup Probe**, altere para:
```yaml
startupProbe:
  timeoutSeconds: 10
  periodSeconds: 10
  failureThreshold: 30
  httpGet:
    path: /api/tags
    port: 8080
```

4. Verifique se a porta do container est√° definida como **8080**:
```yaml
ports:
- name: http1
  containerPort: 8080
```

5. Clique em **IMPLANTAR**

---

## üß™ Como Testar

### 1. Verificar Logs
```bash
gcloud run services logs tail brh-ollama-dev \
  --project=brh-dev-469211 \
  --region=us-east4
```

**Busque por:**
- ‚úÖ `üöÄ Ollama iniciando no ambiente: sandbox`
- ‚úÖ `üåê Porta: 8080`
- ‚úÖ `üì° OLLAMA_HOST: 0.0.0.0:8080`
- ‚ùå Erros de conex√£o ou "connection refused"

### 2. Testar API de Tags (Lista Modelos)
```bash
# Obter token de autentica√ß√£o
TOKEN=$(gcloud auth print-identity-token)

# Testar endpoint
curl -X GET https://brh-ollama-dev-st6yfcc7kq-uk.a.run.app/api/tags \
  -H "Authorization: Bearer $TOKEN"
```

**Resposta esperada:**
```json
{
  "models": [
    {
      "name": "qwen3:4b",
      "modified_at": "2025-01-21T...",
      "size": 4566948864,
      ...
    }
  ]
}
```

### 3. Testar Gera√ß√£o de Texto
```bash
curl -X POST https://brh-ollama-dev-st6yfcc7kq-uk.a.run.app/api/generate \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3:4b",
    "prompt": "Por que o c√©u √© azul?",
    "stream": false
  }'
```

---

## üìä Verificar Status do Servi√ßo

```bash
gcloud run services describe brh-ollama-dev \
  --project=brh-dev-469211 \
  --region=us-east4 \
  --format=yaml | grep -A 10 "status:"
```

**Procure por:**
- `status: 'True'` em todas as condi√ß√µes (Ready, ConfigurationsReady, RoutesReady)
- Nome da revis√£o mais recente

---

## üîÑ Rebuild Completo (Se Necess√°rio)

Se os problemas persistirem, fa√ßa um rebuild completo:

```bash
# 1. Rebuild da imagem com as corre√ß√µes
gcloud builds submit --config=cloudbuild.sandbox.yaml \
  --project=brh-dev-469211

# 2. Aguardar build concluir (pode demorar 10-15 minutos)

# 3. Verificar logs do build
gcloud builds log $(gcloud builds list --limit=1 --format='value(id)')

# 4. Testar novamente
./redeploy-fix.sh sandbox
```

---

## ‚ùì FAQ

### **P: Por que n√£o posso definir OLLAMA_HOST no YAML?**
R: O Cloud Run injeta a vari√°vel `PORT` dinamicamente. O Dockerfile j√° est√° configurado para usar essa porta via `OLLAMA_HOST=0.0.0.0:${PORT}`. Quando voc√™ for√ßa um valor no YAML, cria um conflito.

### **P: O servi√ßo demora muito para ficar pronto (mais de 5 minutos)**
R: O Ollama pode demorar para carregar modelos grandes (especialmente qwen3:4b). Isso √© normal na primeira inicializa√ß√£o. Use `--min-instances=1` para manter uma inst√¢ncia sempre ativa.

### **P: Erro "connection refused" nos logs**
R: Provavelmente o Ollama est√° tentando escutar na porta errada. Verifique se voc√™ **removeu** a vari√°vel `OLLAMA_HOST` do YAML.

### **P: Erro 503 Service Unavailable**
R: O servi√ßo ainda est√° inicializando. Aguarde 2-3 minutos e tente novamente. Verifique os logs para acompanhar o progresso.

### **P: Como saber se o modelo foi carregado corretamente?**
R: Use o endpoint `/api/tags` para listar os modelos dispon√≠veis. Se `qwen3:4b` aparecer, o modelo foi carregado com sucesso.

---

## üìû Suporte Adicional

Se os problemas persistirem:

1. Execute: `./redeploy-fix.sh sandbox`
2. Capture os logs: `gcloud run services logs tail brh-ollama-dev --project=brh-dev-469211 > logs.txt`
3. Teste o endpoint: `curl -v https://brh-ollama-dev-st6yfcc7kq-uk.a.run.app/api/tags`
4. Compartilhe os logs e a sa√≠da do curl para an√°lise
